{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project summarize reports\n",
    "\n",
    "### Objective:\n",
    "- In this project we will combine several Brazilian monetary policy reports provided by the Central Bank and generate a summary of the 3 publications using NLTK resources with the aim of creating a broad summary document of 3 quarters that can be read and researched quickly on all the subject matter covered in the last 9 months on the Brazilian economic scenario.\n",
    "\n",
    "- Nesse projeto vamos unir vários relatórios de política monetária do Brasil fornecido pelo Banco Central e gerar um resumo das 3 publicações utilizando recursos de NLTK com o objetivo de criar um documento resumido amplo de 3 trimestres que possa ser lido e pesquisado de forma rápida sobre todo o assunto tratado nos últimos 9 meses sobre o cenário econômico brasileiro.\n",
    "\n",
    "### Data Origin: https://www.bcb.gov.br/publicacoes/rpm/cronologicos\n",
    "- relatórios de política monetária em PDF: \n",
    "politica_monetaria_set2024.pdf\n",
    "politica_monetaria_dez2024.pdf\n",
    "politica_monetaria_mar2025.pdf\n",
    "\n",
    "- The monetary policy report or inflation report is generated by the Central Bank of Brazil every quarter and contains content of great interest to the market:\n",
    "    - It presents a detailed analysis of the Brazilian and international economic scenario.\n",
    "    - It discloses the BCB's projections for inflation in the short, medium and long term, considering different scenarios.\n",
    "    - It explains the reasons behind the decisions taken by the Monetary Policy Committee (COPOM), especially in relation to the basic interest rate (Selic).\n",
    "    - It provides transparency to the actions and analyses of the Central Bank for the general public, markets and specialists.\n",
    "\n",
    "- O relatório de política monetária ou relatório de inflação é gerado pelo Banco Central do Brasil a cada trimestre e possui um conteúdo de grande interesse do mercado:\n",
    "    - Apresenta uma análise detalhada do cenário econômico brasileiro e internacional. \n",
    "    - Divulga as projeções do BCB para a inflação no curto, médio e longo prazo, considerando diferentes cenários. \n",
    "    - Explica as razões por trás das decisões tomadas pelo Comitê de Política Monetária (COPOM), especialmente em \n",
    "    - relação à taxa básica de juros (Selic). \n",
    "    - Dá transparência às ações e análises do Banco Central para o público em geral, mercados e especialistas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Análise exploratória dos dados\n",
    "- ## Preparação dos dados\n",
    "- ## Armazenamento dos dados tratados\n",
    "- ## Geração de sumário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximiza nro de linhas e colunas para exibição\n",
    "# inibe mensagens de warning\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None) # permite a máxima visualização das linhas em um display\n",
    "pd.set_option('display.max_columns', None) # permite a máxima visualização das colunas em um display\n",
    "import warnings\n",
    "warnings.simplefilter('ignore') # inibe a exibição de avisos de warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte pdf para txt\n",
    "def pdf_to_text(pdf_path, txt_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        for page_number in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_number)\n",
    "\n",
    "            text = page.get_text()\n",
    "            text_file.write(text)\n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    return text_file\n",
    "\n",
    "pdf_path = \"dataset/politica_monetaria_set2024.pdf\"\n",
    "txt_path = \"treated/politica_monetaria_set2024.txt\"\n",
    "text_file = pdf_to_text(pdf_path, txt_path)\n",
    "\n",
    "pdf_path2 = \"dataset/politica_monetaria_dez2024.pdf\"\n",
    "txt_path2 = \"treated/politica_monetaria_dez2024.txt\"\n",
    "text_file2 = pdf_to_text(pdf_path2, txt_path2)\n",
    "\n",
    "pdf_path3 = \"dataset/politica_monetaria_mar2025.pdf\"\n",
    "txt_path3 = \"treated/politica_monetaria_mar2025.txt\"\n",
    "text_file3 = pdf_to_text(pdf_path3, txt_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria a tabela de frequência de palavras\n",
    "def _create_frequency_table(text_string) -> dict:\n",
    "\n",
    "    stopWords = set(stopwords.words(\"portuguese\"))\n",
    "    words = word_tokenize(text_string)\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = ps.stem(word)\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    return freqTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faz a pontuação das frases\n",
    "def _score_sentences(sentences, freqTable) -> dict:\n",
    "    sentenceValue = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        word_count_in_sentence = (len(word_tokenize(sentence)))\n",
    "        for wordValue in freqTable:\n",
    "            if wordValue in sentence.lower():\n",
    "                if sentence[:10] in sentenceValue:\n",
    "                    sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
    "                else:\n",
    "                    sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
    "\n",
    "        sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] // word_count_in_sentence\n",
    "\n",
    "    return sentenceValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula a pontuação média\n",
    "def _find_average_score(sentenceValue) -> int:\n",
    "    sumValues = 0\n",
    "    for entry in sentenceValue:\n",
    "        sumValues += sentenceValue[entry]\n",
    "\n",
    "    # Average value of a sentence from original text\n",
    "    average = int(sumValues / len(sentenceValue))\n",
    "\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gera o resumo dos textos apurados\n",
    "def _generate_summary(sentences, sentenceValue, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] > (threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "String_com_dados_do_arquivo_aux = ''\n",
    "\n",
    "def le_arquivo(txt_path):\n",
    "    global String_com_dados_do_arquivo_aux\n",
    "\n",
    "    with open(txt_path, encoding=\"utf-8\") as arquivo_lido:        \n",
    "        String_com_dados_do_arquivo_aux = arquivo_lido.read() \n",
    "\n",
    "    return arquivo_lido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo_lido = le_arquivo(txt_path)\n",
    "String_com_dados_do_arquivo = String_com_dados_do_arquivo_aux\n",
    "\n",
    "arquivo_lido2 = le_arquivo(txt_path2)\n",
    "String_com_dados_do_arquivo2 = String_com_dados_do_arquivo_aux\n",
    "\n",
    "arquivo_lido3 = le_arquivo(txt_path3)\n",
    "String_com_dados_do_arquivo3 = String_com_dados_do_arquivo_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='treated/politica_monetaria_set2024.txt' mode='r' encoding='utf-8'>\n",
      "<_io.TextIOWrapper name='treated/politica_monetaria_dez2024.txt' mode='r' encoding='utf-8'>\n",
      "<_io.TextIOWrapper name='treated/politica_monetaria_mar2025.txt' mode='r' encoding='utf-8'>\n"
     ]
    }
   ],
   "source": [
    "print(arquivo_lido)\n",
    "print(arquivo_lido2)\n",
    "print(arquivo_lido3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpa_texto(String_com_dados_do_arquivo):\n",
    "    article_text = \"\"\n",
    "\n",
    "    for line in String_com_dados_do_arquivo:\n",
    "        article_text += line\n",
    "\n",
    "    # Removing Square Brackets and Extra Spaces\n",
    "    article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "    article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "\n",
    "    # Removing special characters and digits\n",
    "    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "\n",
    "    sentence_list = nltk.sent_tokenize(article_text)\n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = limpa_texto(String_com_dados_do_arquivo)\n",
    "article_text2 = limpa_texto(String_com_dados_do_arquivo2)\n",
    "article_text3 = limpa_texto(String_com_dados_do_arquivo3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(article_text)\\nprint(article_text2)\\nprint(article_text3)\\n'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(article_text)\n",
    "print(article_text2)\n",
    "print(article_text3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sumariza_relatorio(article_text):\n",
    "    # 1 Create the word frequency table\n",
    "    freq_table = _create_frequency_table(article_text)\n",
    "\n",
    "    '''\n",
    "    We already have a sentence tokenizer, so we just need \n",
    "    to run the sent_tokenize() method to create the array of sentences.\n",
    "    '''\n",
    "\n",
    "    # 2 Tokenize the sentences\n",
    "    sentences = sent_tokenize(article_text)\n",
    "\n",
    "    # 3 Important Algorithm: score the sentences\n",
    "    sentence_scores = _score_sentences(sentences, freq_table)\n",
    "\n",
    "    # 4 Find the threshold\n",
    "    threshold = _find_average_score(sentence_scores)\n",
    "\n",
    "    # 5 Important Algorithm: Generate the summary\n",
    "    summary = _generate_summary(sentences, sentence_scores, 1.5 * threshold)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = sumariza_relatorio(article_text)\n",
    "summary2 = sumariza_relatorio(article_text2)\n",
    "summary3 = sumariza_relatorio(article_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summary + ' ' + summary2 + ' ' + summary3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grava o sumário em arquivo word\n",
    "# Create a new Document\n",
    "doc = Document()\n",
    "\n",
    "# Add a paragraph with the string\n",
    "doc.add_paragraph(summary)\n",
    "\n",
    "# Save the document\n",
    "doc.save(\"treated/politica_monetaria_set2024_dez2004_mar2025.docx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
